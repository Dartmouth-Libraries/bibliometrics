{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing text files exported from Web of Science Database and Compiling them into a single dataset (and saving as a .csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path   #for working with filepaths\n",
    "import pandas as pd        #for creating and working with dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set path to directory containing WoS Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('c:/Users/F0040RP/Documents/DartLib_RDS/projects/bibliometric-analysis/WoS')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check your current working directory\n",
    "Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['resilience10001-10500.txt',\n",
       " 'resilience1001-1500.txt',\n",
       " 'resilience10501-11000.txt',\n",
       " 'resilience11001-11500.txt',\n",
       " 'resilience11501-12000.txt',\n",
       " 'resilience12001-12500.txt',\n",
       " 'resilience12501-13000.txt',\n",
       " 'resilience13001-13500.txt',\n",
       " 'resilience13501-14000.txt',\n",
       " 'resilience14001-14500.txt',\n",
       " 'resilience14501-15000.txt',\n",
       " 'resilience15001-15500.txt',\n",
       " 'resilience1501-2000.txt',\n",
       " 'resilience15501-16000.txt',\n",
       " 'resilience16001-16500.txt',\n",
       " 'resilience16501-17000.txt',\n",
       " 'resilience17001-17500.txt',\n",
       " 'resilience17501-18000.txt',\n",
       " 'resilience18001-18500.txt',\n",
       " 'resilience18501-19000.txt',\n",
       " 'resilience19001-19500.txt',\n",
       " 'resilience19501-20000.txt',\n",
       " 'resilience1_500.txt',\n",
       " 'resilience20001-20500.txt',\n",
       " 'resilience2001-2500.txt',\n",
       " 'resilience20501-21000.txt',\n",
       " 'resilience21001-21500.txt',\n",
       " 'resilience21501-22000.txt',\n",
       " 'resilience22001-22500.txt',\n",
       " 'resilience22501-23000.txt',\n",
       " 'resilience23001-23500.txt',\n",
       " 'resilience23501-24000.txt',\n",
       " 'resilience24001-24500.txt',\n",
       " 'resilience24501-25000.txt',\n",
       " 'resilience25001-25500.txt',\n",
       " 'resilience2501-3000.txt',\n",
       " 'resilience25501-26000.txt',\n",
       " 'resilience26001-26500.txt',\n",
       " 'resilience26501-27000.txt',\n",
       " 'resilience27001-27500.txt',\n",
       " 'resilience27501-28000.txt',\n",
       " 'resilience28001-28500.txt',\n",
       " 'resilience28501-29000.txt',\n",
       " 'resilience29001-29500.txt',\n",
       " 'resilience29501-30000.txt',\n",
       " 'resilience30001-30500.txt',\n",
       " 'resilience3001-3500.txt',\n",
       " 'resilience30501-31000.txt',\n",
       " 'resilience31001-31500.txt',\n",
       " 'resilience31501-32000.txt',\n",
       " 'resilience32001-32500.txt',\n",
       " 'resilience32501-33000.txt',\n",
       " 'resilience33001-33500.txt',\n",
       " 'resilience33501-34000.txt',\n",
       " 'resilience34001-34500.txt',\n",
       " 'resilience34501-35000.txt',\n",
       " 'resilience35001-35500.txt',\n",
       " 'resilience3501-4000.txt',\n",
       " 'resilience35501-36000.txt',\n",
       " 'resilience36001-36500.txt',\n",
       " 'resilience36501-37000.txt',\n",
       " 'resilience37001-37500.txt',\n",
       " 'resilience37501-38000.txt',\n",
       " 'resilience38001-38500.txt',\n",
       " 'resilience38501-39000.txt',\n",
       " 'resilience39001-39500.txt',\n",
       " 'resilience39501-40000.txt',\n",
       " 'resilience40001-40500.txt',\n",
       " 'resilience4001-4500.txt',\n",
       " 'resilience40501-41000.txt',\n",
       " 'resilience41001-41500.txt',\n",
       " 'resilience41501-42000.txt',\n",
       " 'resilience42001-42500.txt',\n",
       " 'resilience42501-43000.txt',\n",
       " 'resilience43001-43500.txt',\n",
       " 'resilience43501-44000.txt',\n",
       " 'resilience44001-44500.txt',\n",
       " 'resilience44501-45000.txt',\n",
       " 'resilience45001-45500.txt',\n",
       " 'resilience4501-5000.txt',\n",
       " 'resilience45501-46000.txt',\n",
       " 'resilience46001-46500.txt',\n",
       " 'resilience46501-47000.txt',\n",
       " 'resilience47001-47500.txt',\n",
       " 'resilience47501-48000.txt',\n",
       " 'resilience48001-48500.txt',\n",
       " 'resilience48501-49000.txt',\n",
       " 'resilience49001-49500.txt',\n",
       " 'resilience49501-50000.txt',\n",
       " 'resilience50001-50500.txt',\n",
       " 'resilience5001-5500.txt',\n",
       " 'resilience501-1000.txt',\n",
       " 'resilience50501-51000.txt',\n",
       " 'resilience51001-51500.txt',\n",
       " 'resilience51501-52000.txt',\n",
       " 'resilience52001-52500.txt',\n",
       " 'resilience52501-53000.txt',\n",
       " 'resilience53001-53500.txt',\n",
       " 'resilience53501-54000.txt',\n",
       " 'resilience54001-54500.txt',\n",
       " 'resilience54501-55000.txt',\n",
       " 'resilience55001-55500.txt',\n",
       " 'resilience5501-6000.txt',\n",
       " 'resilience55501-56000.txt',\n",
       " 'resilience56001-56500.txt',\n",
       " 'resilience56501-57000.txt',\n",
       " 'resilience57001-57500.txt',\n",
       " 'resilience57501-58000.txt',\n",
       " 'resilience58001-58500.txt',\n",
       " 'resilience58501-59000.txt',\n",
       " 'resilience59001-59500.txt',\n",
       " 'resilience59501-60000.txt',\n",
       " 'resilience60001-60500.txt',\n",
       " 'resilience6001-6500.txt',\n",
       " 'resilience60501-61000.txt',\n",
       " 'resilience61001-61500.txt',\n",
       " 'resilience61501-62000.txt',\n",
       " 'resilience62001-62500.txt',\n",
       " 'resilience62501-63000.txt',\n",
       " 'resilience63001-63500.txt',\n",
       " 'resilience63501-64000.txt',\n",
       " 'resilience64001-64500.txt',\n",
       " 'resilience64501-65000.txt',\n",
       " 'resilience65001-65500.txt',\n",
       " 'resilience6501-7000.txt',\n",
       " 'resilience65501-66000.txt',\n",
       " 'resilience66001-66500.txt',\n",
       " 'resilience66501-67000.txt',\n",
       " 'resilience67001-67500.txt',\n",
       " 'resilience67501-68000.txt',\n",
       " 'resilience68001-68500.txt',\n",
       " 'resilience68501-69000.txt',\n",
       " 'resilience69001-69500.txt',\n",
       " 'resilience69501-70000.txt',\n",
       " 'resilience70001-70500.txt',\n",
       " 'resilience7001-7500.txt',\n",
       " 'resilience70501-71000.txt',\n",
       " 'resilience71001-71500.txt',\n",
       " 'resilience71501-72000.txt',\n",
       " 'resilience72001-72500.txt',\n",
       " 'resilience72501-73000.txt',\n",
       " 'resilience73001-73500.txt',\n",
       " 'resilience73501-74000.txt',\n",
       " 'resilience74001-74500.txt',\n",
       " 'resilience74501-75000.txt',\n",
       " 'resilience75001-75500.txt',\n",
       " 'resilience7501-8000.txt',\n",
       " 'resilience75501-76000.txt',\n",
       " 'resilience76001-76500.txt',\n",
       " 'resilience76501-77000.txt',\n",
       " 'resilience77001-77500.txt',\n",
       " 'resilience77501-78000.txt',\n",
       " 'resilience78001-78500.txt',\n",
       " 'resilience78501-79000.txt',\n",
       " 'resilience79001-79500.txt',\n",
       " 'resilience79501-80000.txt',\n",
       " 'resilience80001-80500.txt',\n",
       " 'resilience8001-8500.txt',\n",
       " 'resilience80501-81000.txt',\n",
       " 'resilience81001-81500.txt',\n",
       " 'resilience81501-82000.txt',\n",
       " 'resilience82001-82500.txt',\n",
       " 'resilience82501-83000.txt',\n",
       " 'resilience83001-83500.txt',\n",
       " 'resilience83501-84000.txt',\n",
       " 'resilience84001-84500.txt',\n",
       " 'resilience84501-85000.txt',\n",
       " 'resilience85001-85500.txt',\n",
       " 'resilience8501-9000.txt',\n",
       " 'resilience85501-86000.txt',\n",
       " 'resilience86001-86500.txt',\n",
       " 'resilience86501-87000.txt',\n",
       " 'resilience87001-87500.txt',\n",
       " 'resilience87501-88000.txt',\n",
       " 'resilience88001-88500.txt',\n",
       " 'resilience88501-89000.txt',\n",
       " 'resilience89001-89500.txt',\n",
       " 'resilience89501-90000.txt',\n",
       " 'resilience90001-90500.txt',\n",
       " 'resilience9001-9500.txt',\n",
       " 'resilience90501-91000.txt',\n",
       " 'resilience91001-91500.txt',\n",
       " 'resilience91501-92000.txt',\n",
       " 'resilience92001-92500.txt',\n",
       " 'resilience92501-93000.txt',\n",
       " 'resilience93001-93500.txt',\n",
       " 'resilience93501-94000.txt',\n",
       " 'resilience94001-94500.txt',\n",
       " 'resilience94501-95000.txt',\n",
       " 'resilience95001-95500.txt',\n",
       " 'resilience9501-10000.txt',\n",
       " 'resilience95501-96000.txt',\n",
       " 'resilience96001-96500.txt',\n",
       " 'resilience96501-97000.txt',\n",
       " 'resilience97001-97500.txt',\n",
       " 'resilience97501-98000.txt',\n",
       " 'resilience98001-98500.txt',\n",
       " 'resilience98501-99000.txt',\n",
       " 'resilience99001-99500.txt',\n",
       " 'resilience99501-100000.txt']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pathdir = Path.cwd()                        #use this if tsv .txt files are in current working directory\n",
    "pathdir = Path(\"resilience/.txt files\")                 #use this to set relative path to .txt files if they are not in cwd\n",
    "\n",
    "#pathdir = Path(\"WoS/resilience\")         \n",
    "pathlist = sorted(pathdir.glob('*.txt'))    \n",
    "[path.name for path in pathlist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Read in tab-separated-value text files and combine into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading & appending file number: 0 (with 500 rows) of 200 total files. Pathname:  resilience10001-10500\n",
      "Reading & appending file number: 1 (with 500 rows) of 200 total files. Pathname:  resilience1001-1500\n",
      "Reading & appending file number: 2 (with 500 rows) of 200 total files. Pathname:  resilience10501-11000\n",
      "Reading & appending file number: 3 (with 500 rows) of 200 total files. Pathname:  resilience11001-11500\n",
      "Reading & appending file number: 4 (with 500 rows) of 200 total files. Pathname:  resilience11501-12000\n",
      "Reading & appending file number: 5 (with 500 rows) of 200 total files. Pathname:  resilience12001-12500\n",
      "Reading & appending file number: 6 (with 500 rows) of 200 total files. Pathname:  resilience12501-13000\n",
      "Reading & appending file number: 7 (with 500 rows) of 200 total files. Pathname:  resilience13001-13500\n",
      "Reading & appending file number: 8 (with 500 rows) of 200 total files. Pathname:  resilience13501-14000\n",
      "Reading & appending file number: 9 (with 500 rows) of 200 total files. Pathname:  resilience14001-14500\n",
      "Reading & appending file number: 10 (with 500 rows) of 200 total files. Pathname:  resilience14501-15000\n",
      "Reading & appending file number: 11 (with 500 rows) of 200 total files. Pathname:  resilience15001-15500\n",
      "Reading & appending file number: 12 (with 500 rows) of 200 total files. Pathname:  resilience1501-2000\n",
      "Reading & appending file number: 13 (with 500 rows) of 200 total files. Pathname:  resilience15501-16000\n",
      "Reading & appending file number: 14 (with 500 rows) of 200 total files. Pathname:  resilience16001-16500\n",
      "Reading & appending file number: 15 (with 500 rows) of 200 total files. Pathname:  resilience16501-17000\n",
      "Reading & appending file number: 16 (with 500 rows) of 200 total files. Pathname:  resilience17001-17500\n",
      "Reading & appending file number: 17 (with 500 rows) of 200 total files. Pathname:  resilience17501-18000\n",
      "Reading & appending file number: 18 (with 500 rows) of 200 total files. Pathname:  resilience18001-18500\n",
      "Reading & appending file number: 19 (with 494 rows) of 200 total files. Pathname:  resilience18501-19000\n",
      "Reading & appending file number: 20 (with 500 rows) of 200 total files. Pathname:  resilience19001-19500\n",
      "Reading & appending file number: 21 (with 498 rows) of 200 total files. Pathname:  resilience19501-20000\n",
      "Reading & appending file number: 22 (with 500 rows) of 200 total files. Pathname:  resilience1_500\n",
      "Reading & appending file number: 23 (with 487 rows) of 200 total files. Pathname:  resilience20001-20500\n",
      "Reading & appending file number: 24 (with 500 rows) of 200 total files. Pathname:  resilience2001-2500\n",
      "Reading & appending file number: 25 (with 500 rows) of 200 total files. Pathname:  resilience20501-21000\n",
      "Reading & appending file number: 26 (with 500 rows) of 200 total files. Pathname:  resilience21001-21500\n",
      "Reading & appending file number: 27 (with 498 rows) of 200 total files. Pathname:  resilience21501-22000\n",
      "Reading & appending file number: 28 (with 500 rows) of 200 total files. Pathname:  resilience22001-22500\n",
      "Reading & appending file number: 29 (with 500 rows) of 200 total files. Pathname:  resilience22501-23000\n",
      "Reading & appending file number: 30 (with 500 rows) of 200 total files. Pathname:  resilience23001-23500\n",
      "Reading & appending file number: 31 (with 497 rows) of 200 total files. Pathname:  resilience23501-24000\n",
      "Reading & appending file number: 32 (with 500 rows) of 200 total files. Pathname:  resilience24001-24500\n",
      "Reading & appending file number: 33 (with 490 rows) of 200 total files. Pathname:  resilience24501-25000\n",
      "Reading & appending file number: 34 (with 500 rows) of 200 total files. Pathname:  resilience25001-25500\n",
      "Reading & appending file number: 35 (with 500 rows) of 200 total files. Pathname:  resilience2501-3000\n",
      "Reading & appending file number: 36 (with 500 rows) of 200 total files. Pathname:  resilience25501-26000\n",
      "Reading & appending file number: 37 (with 500 rows) of 200 total files. Pathname:  resilience26001-26500\n",
      "Reading & appending file number: 38 (with 500 rows) of 200 total files. Pathname:  resilience26501-27000\n",
      "Reading & appending file number: 39 (with 500 rows) of 200 total files. Pathname:  resilience27001-27500\n",
      "Reading & appending file number: 40 (with 500 rows) of 200 total files. Pathname:  resilience27501-28000\n",
      "Reading & appending file number: 41 (with 500 rows) of 200 total files. Pathname:  resilience28001-28500\n",
      "Reading & appending file number: 42 (with 500 rows) of 200 total files. Pathname:  resilience28501-29000\n",
      "Reading & appending file number: 43 (with 500 rows) of 200 total files. Pathname:  resilience29001-29500\n",
      "Reading & appending file number: 44 (with 500 rows) of 200 total files. Pathname:  resilience29501-30000\n",
      "Reading & appending file number: 45 (with 500 rows) of 200 total files. Pathname:  resilience30001-30500\n",
      "Reading & appending file number: 46 (with 500 rows) of 200 total files. Pathname:  resilience3001-3500\n",
      "Reading & appending file number: 47 (with 500 rows) of 200 total files. Pathname:  resilience30501-31000\n",
      "Reading & appending file number: 48 (with 500 rows) of 200 total files. Pathname:  resilience31001-31500\n",
      "Reading & appending file number: 49 (with 500 rows) of 200 total files. Pathname:  resilience31501-32000\n",
      "Reading & appending file number: 50 (with 500 rows) of 200 total files. Pathname:  resilience32001-32500\n",
      "Reading & appending file number: 51 (with 495 rows) of 200 total files. Pathname:  resilience32501-33000\n",
      "Reading & appending file number: 52 (with 500 rows) of 200 total files. Pathname:  resilience33001-33500\n",
      "Reading & appending file number: 53 (with 500 rows) of 200 total files. Pathname:  resilience33501-34000\n",
      "Reading & appending file number: 54 (with 488 rows) of 200 total files. Pathname:  resilience34001-34500\n",
      "Reading & appending file number: 55 (with 500 rows) of 200 total files. Pathname:  resilience34501-35000\n",
      "Reading & appending file number: 56 (with 500 rows) of 200 total files. Pathname:  resilience35001-35500\n",
      "Reading & appending file number: 57 (with 500 rows) of 200 total files. Pathname:  resilience3501-4000\n",
      "Reading & appending file number: 58 (with 500 rows) of 200 total files. Pathname:  resilience35501-36000\n",
      "Reading & appending file number: 59 (with 500 rows) of 200 total files. Pathname:  resilience36001-36500\n",
      "Reading & appending file number: 60 (with 483 rows) of 200 total files. Pathname:  resilience36501-37000\n",
      "Reading & appending file number: 61 (with 496 rows) of 200 total files. Pathname:  resilience37001-37500\n",
      "Reading & appending file number: 62 (with 500 rows) of 200 total files. Pathname:  resilience37501-38000\n",
      "Reading & appending file number: 63 (with 500 rows) of 200 total files. Pathname:  resilience38001-38500\n",
      "Reading & appending file number: 64 (with 500 rows) of 200 total files. Pathname:  resilience38501-39000\n",
      "Reading & appending file number: 65 (with 500 rows) of 200 total files. Pathname:  resilience39001-39500\n",
      "Reading & appending file number: 66 (with 500 rows) of 200 total files. Pathname:  resilience39501-40000\n",
      "Reading & appending file number: 67 (with 500 rows) of 200 total files. Pathname:  resilience40001-40500\n",
      "Reading & appending file number: 68 (with 500 rows) of 200 total files. Pathname:  resilience4001-4500\n",
      "Reading & appending file number: 69 (with 500 rows) of 200 total files. Pathname:  resilience40501-41000\n",
      "Reading & appending file number: 70 (with 500 rows) of 200 total files. Pathname:  resilience41001-41500\n",
      "Reading & appending file number: 71 (with 500 rows) of 200 total files. Pathname:  resilience41501-42000\n",
      "Reading & appending file number: 72 (with 498 rows) of 200 total files. Pathname:  resilience42001-42500\n",
      "Reading & appending file number: 73 (with 500 rows) of 200 total files. Pathname:  resilience42501-43000\n",
      "Reading & appending file number: 74 (with 500 rows) of 200 total files. Pathname:  resilience43001-43500\n",
      "Reading & appending file number: 75 (with 500 rows) of 200 total files. Pathname:  resilience43501-44000\n",
      "Reading & appending file number: 76 (with 500 rows) of 200 total files. Pathname:  resilience44001-44500\n",
      "Reading & appending file number: 77 (with 500 rows) of 200 total files. Pathname:  resilience44501-45000\n",
      "Reading & appending file number: 78 (with 500 rows) of 200 total files. Pathname:  resilience45001-45500\n",
      "Reading & appending file number: 79 (with 500 rows) of 200 total files. Pathname:  resilience4501-5000\n",
      "Reading & appending file number: 80 (with 487 rows) of 200 total files. Pathname:  resilience45501-46000\n",
      "Reading & appending file number: 81 (with 500 rows) of 200 total files. Pathname:  resilience46001-46500\n",
      "Reading & appending file number: 82 (with 500 rows) of 200 total files. Pathname:  resilience46501-47000\n",
      "Reading & appending file number: 83 (with 500 rows) of 200 total files. Pathname:  resilience47001-47500\n",
      "Reading & appending file number: 84 (with 498 rows) of 200 total files. Pathname:  resilience47501-48000\n",
      "Reading & appending file number: 85 (with 500 rows) of 200 total files. Pathname:  resilience48001-48500\n",
      "Reading & appending file number: 86 (with 500 rows) of 200 total files. Pathname:  resilience48501-49000\n",
      "Reading & appending file number: 87 (with 496 rows) of 200 total files. Pathname:  resilience49001-49500\n",
      "Reading & appending file number: 88 (with 494 rows) of 200 total files. Pathname:  resilience49501-50000\n",
      "Reading & appending file number: 89 (with 500 rows) of 200 total files. Pathname:  resilience50001-50500\n",
      "Reading & appending file number: 90 (with 500 rows) of 200 total files. Pathname:  resilience5001-5500\n",
      "Reading & appending file number: 91 (with 500 rows) of 200 total files. Pathname:  resilience501-1000\n",
      "Reading & appending file number: 92 (with 500 rows) of 200 total files. Pathname:  resilience50501-51000\n",
      "Reading & appending file number: 93 (with 500 rows) of 200 total files. Pathname:  resilience51001-51500\n",
      "Reading & appending file number: 94 (with 500 rows) of 200 total files. Pathname:  resilience51501-52000\n",
      "Reading & appending file number: 95 (with 492 rows) of 200 total files. Pathname:  resilience52001-52500\n",
      "Reading & appending file number: 96 (with 500 rows) of 200 total files. Pathname:  resilience52501-53000\n",
      "Reading & appending file number: 97 (with 500 rows) of 200 total files. Pathname:  resilience53001-53500\n",
      "Reading & appending file number: 98 (with 500 rows) of 200 total files. Pathname:  resilience53501-54000\n",
      "Reading & appending file number: 99 (with 500 rows) of 200 total files. Pathname:  resilience54001-54500\n",
      "Reading & appending file number: 100 (with 500 rows) of 200 total files. Pathname:  resilience54501-55000\n",
      "Reading & appending file number: 101 (with 500 rows) of 200 total files. Pathname:  resilience55001-55500\n",
      "Reading & appending file number: 102 (with 500 rows) of 200 total files. Pathname:  resilience5501-6000\n",
      "Reading & appending file number: 103 (with 500 rows) of 200 total files. Pathname:  resilience55501-56000\n",
      "Reading & appending file number: 104 (with 498 rows) of 200 total files. Pathname:  resilience56001-56500\n",
      "Reading & appending file number: 105 (with 500 rows) of 200 total files. Pathname:  resilience56501-57000\n",
      "Reading & appending file number: 106 (with 465 rows) of 200 total files. Pathname:  resilience57001-57500\n",
      "Reading & appending file number: 107 (with 500 rows) of 200 total files. Pathname:  resilience57501-58000\n",
      "Reading & appending file number: 108 (with 500 rows) of 200 total files. Pathname:  resilience58001-58500\n",
      "Reading & appending file number: 109 (with 500 rows) of 200 total files. Pathname:  resilience58501-59000\n",
      "Reading & appending file number: 110 (with 493 rows) of 200 total files. Pathname:  resilience59001-59500\n",
      "Reading & appending file number: 111 (with 500 rows) of 200 total files. Pathname:  resilience59501-60000\n",
      "Reading & appending file number: 112 (with 500 rows) of 200 total files. Pathname:  resilience60001-60500\n",
      "Reading & appending file number: 113 (with 500 rows) of 200 total files. Pathname:  resilience6001-6500\n",
      "Reading & appending file number: 114 (with 500 rows) of 200 total files. Pathname:  resilience60501-61000\n",
      "Reading & appending file number: 115 (with 500 rows) of 200 total files. Pathname:  resilience61001-61500\n",
      "Reading & appending file number: 116 (with 500 rows) of 200 total files. Pathname:  resilience61501-62000\n",
      "Reading & appending file number: 117 (with 500 rows) of 200 total files. Pathname:  resilience62001-62500\n",
      "Reading & appending file number: 118 (with 490 rows) of 200 total files. Pathname:  resilience62501-63000\n",
      "Reading & appending file number: 119 (with 500 rows) of 200 total files. Pathname:  resilience63001-63500\n",
      "Reading & appending file number: 120 (with 500 rows) of 200 total files. Pathname:  resilience63501-64000\n",
      "Reading & appending file number: 121 (with 499 rows) of 200 total files. Pathname:  resilience64001-64500\n",
      "Reading & appending file number: 122 (with 500 rows) of 200 total files. Pathname:  resilience64501-65000\n",
      "Reading & appending file number: 123 (with 500 rows) of 200 total files. Pathname:  resilience65001-65500\n",
      "Reading & appending file number: 124 (with 500 rows) of 200 total files. Pathname:  resilience6501-7000\n",
      "Reading & appending file number: 125 (with 490 rows) of 200 total files. Pathname:  resilience65501-66000\n",
      "Reading & appending file number: 126 (with 500 rows) of 200 total files. Pathname:  resilience66001-66500\n",
      "Reading & appending file number: 127 (with 500 rows) of 200 total files. Pathname:  resilience66501-67000\n",
      "Reading & appending file number: 128 (with 500 rows) of 200 total files. Pathname:  resilience67001-67500\n",
      "Reading & appending file number: 129 (with 500 rows) of 200 total files. Pathname:  resilience67501-68000\n",
      "Reading & appending file number: 130 (with 500 rows) of 200 total files. Pathname:  resilience68001-68500\n",
      "Reading & appending file number: 131 (with 500 rows) of 200 total files. Pathname:  resilience68501-69000\n",
      "Reading & appending file number: 132 (with 500 rows) of 200 total files. Pathname:  resilience69001-69500\n",
      "Reading & appending file number: 133 (with 500 rows) of 200 total files. Pathname:  resilience69501-70000\n",
      "Reading & appending file number: 134 (with 500 rows) of 200 total files. Pathname:  resilience70001-70500\n",
      "Reading & appending file number: 135 (with 500 rows) of 200 total files. Pathname:  resilience7001-7500\n",
      "Reading & appending file number: 136 (with 500 rows) of 200 total files. Pathname:  resilience70501-71000\n",
      "Reading & appending file number: 137 (with 500 rows) of 200 total files. Pathname:  resilience71001-71500\n",
      "Reading & appending file number: 138 (with 500 rows) of 200 total files. Pathname:  resilience71501-72000\n",
      "Reading & appending file number: 139 (with 500 rows) of 200 total files. Pathname:  resilience72001-72500\n",
      "Reading & appending file number: 140 (with 500 rows) of 200 total files. Pathname:  resilience72501-73000\n",
      "Reading & appending file number: 141 (with 500 rows) of 200 total files. Pathname:  resilience73001-73500\n",
      "Reading & appending file number: 142 (with 500 rows) of 200 total files. Pathname:  resilience73501-74000\n",
      "Reading & appending file number: 143 (with 500 rows) of 200 total files. Pathname:  resilience74001-74500\n",
      "Reading & appending file number: 144 (with 500 rows) of 200 total files. Pathname:  resilience74501-75000\n",
      "Reading & appending file number: 145 (with 500 rows) of 200 total files. Pathname:  resilience75001-75500\n",
      "Reading & appending file number: 146 (with 500 rows) of 200 total files. Pathname:  resilience7501-8000\n",
      "Reading & appending file number: 147 (with 500 rows) of 200 total files. Pathname:  resilience75501-76000\n",
      "Reading & appending file number: 148 (with 500 rows) of 200 total files. Pathname:  resilience76001-76500\n",
      "Reading & appending file number: 149 (with 489 rows) of 200 total files. Pathname:  resilience76501-77000\n",
      "Reading & appending file number: 150 (with 500 rows) of 200 total files. Pathname:  resilience77001-77500\n",
      "Reading & appending file number: 151 (with 500 rows) of 200 total files. Pathname:  resilience77501-78000\n",
      "Reading & appending file number: 152 (with 500 rows) of 200 total files. Pathname:  resilience78001-78500\n",
      "Reading & appending file number: 153 (with 500 rows) of 200 total files. Pathname:  resilience78501-79000\n",
      "Reading & appending file number: 154 (with 500 rows) of 200 total files. Pathname:  resilience79001-79500\n",
      "Reading & appending file number: 155 (with 500 rows) of 200 total files. Pathname:  resilience79501-80000\n",
      "Reading & appending file number: 156 (with 500 rows) of 200 total files. Pathname:  resilience80001-80500\n",
      "Reading & appending file number: 157 (with 500 rows) of 200 total files. Pathname:  resilience8001-8500\n",
      "Reading & appending file number: 158 (with 500 rows) of 200 total files. Pathname:  resilience80501-81000\n",
      "Reading & appending file number: 159 (with 497 rows) of 200 total files. Pathname:  resilience81001-81500\n",
      "Reading & appending file number: 160 (with 493 rows) of 200 total files. Pathname:  resilience81501-82000\n",
      "Reading & appending file number: 161 (with 500 rows) of 200 total files. Pathname:  resilience82001-82500\n",
      "Reading & appending file number: 162 (with 500 rows) of 200 total files. Pathname:  resilience82501-83000\n",
      "Reading & appending file number: 163 (with 500 rows) of 200 total files. Pathname:  resilience83001-83500\n",
      "Reading & appending file number: 164 (with 500 rows) of 200 total files. Pathname:  resilience83501-84000\n",
      "Reading & appending file number: 165 (with 500 rows) of 200 total files. Pathname:  resilience84001-84500\n",
      "Reading & appending file number: 166 (with 500 rows) of 200 total files. Pathname:  resilience84501-85000\n",
      "Reading & appending file number: 167 (with 500 rows) of 200 total files. Pathname:  resilience85001-85500\n",
      "Reading & appending file number: 168 (with 500 rows) of 200 total files. Pathname:  resilience8501-9000\n",
      "Reading & appending file number: 169 (with 499 rows) of 200 total files. Pathname:  resilience85501-86000\n",
      "Reading & appending file number: 170 (with 500 rows) of 200 total files. Pathname:  resilience86001-86500\n",
      "Reading & appending file number: 171 (with 500 rows) of 200 total files. Pathname:  resilience86501-87000\n",
      "Reading & appending file number: 172 (with 500 rows) of 200 total files. Pathname:  resilience87001-87500\n",
      "Reading & appending file number: 173 (with 500 rows) of 200 total files. Pathname:  resilience87501-88000\n",
      "Reading & appending file number: 174 (with 500 rows) of 200 total files. Pathname:  resilience88001-88500\n",
      "Reading & appending file number: 175 (with 473 rows) of 200 total files. Pathname:  resilience88501-89000\n",
      "Reading & appending file number: 176 (with 500 rows) of 200 total files. Pathname:  resilience89001-89500\n",
      "Reading & appending file number: 177 (with 499 rows) of 200 total files. Pathname:  resilience89501-90000\n",
      "Reading & appending file number: 178 (with 500 rows) of 200 total files. Pathname:  resilience90001-90500\n",
      "Reading & appending file number: 179 (with 500 rows) of 200 total files. Pathname:  resilience9001-9500\n",
      "Reading & appending file number: 180 (with 500 rows) of 200 total files. Pathname:  resilience90501-91000\n",
      "Reading & appending file number: 181 (with 500 rows) of 200 total files. Pathname:  resilience91001-91500\n",
      "Reading & appending file number: 182 (with 500 rows) of 200 total files. Pathname:  resilience91501-92000\n",
      "Reading & appending file number: 183 (with 500 rows) of 200 total files. Pathname:  resilience92001-92500\n",
      "Reading & appending file number: 184 (with 500 rows) of 200 total files. Pathname:  resilience92501-93000\n",
      "Reading & appending file number: 185 (with 500 rows) of 200 total files. Pathname:  resilience93001-93500\n",
      "Reading & appending file number: 186 (with 500 rows) of 200 total files. Pathname:  resilience93501-94000\n",
      "Reading & appending file number: 187 (with 500 rows) of 200 total files. Pathname:  resilience94001-94500\n",
      "Reading & appending file number: 188 (with 500 rows) of 200 total files. Pathname:  resilience94501-95000\n",
      "Reading & appending file number: 189 (with 500 rows) of 200 total files. Pathname:  resilience95001-95500\n",
      "Reading & appending file number: 190 (with 500 rows) of 200 total files. Pathname:  resilience9501-10000\n",
      "Reading & appending file number: 191 (with 500 rows) of 200 total files. Pathname:  resilience95501-96000\n",
      "Reading & appending file number: 192 (with 499 rows) of 200 total files. Pathname:  resilience96001-96500\n",
      "Reading & appending file number: 193 (with 495 rows) of 200 total files. Pathname:  resilience96501-97000\n",
      "Reading & appending file number: 194 (with 500 rows) of 200 total files. Pathname:  resilience97001-97500\n",
      "Reading & appending file number: 195 (with 492 rows) of 200 total files. Pathname:  resilience97501-98000\n",
      "Reading & appending file number: 196 (with 496 rows) of 200 total files. Pathname:  resilience98001-98500\n",
      "Reading & appending file number: 197 (with 500 rows) of 200 total files. Pathname:  resilience98501-99000\n",
      "Reading & appending file number: 198 (with 500 rows) of 200 total files. Pathname:  resilience99001-99500\n",
      "Reading & appending file number: 199 (with 500 rows) of 200 total files. Pathname:  resilience99501-100000\n"
     ]
    }
   ],
   "source": [
    "datalist = []                           #creates an empty list\n",
    "for i, path in enumerate(pathlist):\n",
    "    df = pd.read_csv(path, sep = \"\\t\")\n",
    "    print(\"Reading & appending file number:\", i, \"(with %s rows) of\" %df.shape[0], len(pathlist), \"total files. Pathname: \", path.stem)\n",
    "    datalist.append(df)    ## appends each imported dataframe into a list of dataframes\n",
    "    \n",
    "data = pd.concat(datalist)        #concatenates or joins each dataframe in datalist into one dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Get summary information for the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99758, 71)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PT</th>\n",
       "      <th>AU</th>\n",
       "      <th>BA</th>\n",
       "      <th>BE</th>\n",
       "      <th>GP</th>\n",
       "      <th>AF</th>\n",
       "      <th>BF</th>\n",
       "      <th>CA</th>\n",
       "      <th>TI</th>\n",
       "      <th>SO</th>\n",
       "      <th>...</th>\n",
       "      <th>WC</th>\n",
       "      <th>WE</th>\n",
       "      <th>SC</th>\n",
       "      <th>GA</th>\n",
       "      <th>PM</th>\n",
       "      <th>OA</th>\n",
       "      <th>HC</th>\n",
       "      <th>HP</th>\n",
       "      <th>DA</th>\n",
       "      <th>UT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>J</td>\n",
       "      <td>Sun, XY; Dai, XY; Yang, TS; Song, HT; Yang, JL...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sun, Xinyang; Dai, Xuyan; Yang, Tingshu; Song,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Effects of mental resilience on neuroendocrine...</td>\n",
       "      <td>ENDOCRINE</td>\n",
       "      <td>...</td>\n",
       "      <td>Endocrinology &amp; Metabolism</td>\n",
       "      <td>Science Citation Index Expanded (SCI-EXPANDED)</td>\n",
       "      <td>Endocrinology &amp; Metabolism</td>\n",
       "      <td>AU1UG</td>\n",
       "      <td>24633577.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-05</td>\n",
       "      <td>WOS:000345404900029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>J</td>\n",
       "      <td>Ladanyi, A; Cinkler, T</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ladanyi, Akos; Cinkler, Tibor</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Resilience-throughput-power trade-off in futur...</td>\n",
       "      <td>PHOTONIC NETWORK COMMUNICATIONS</td>\n",
       "      <td>...</td>\n",
       "      <td>Computer Science, Information Systems; Optics;...</td>\n",
       "      <td>Science Citation Index Expanded (SCI-EXPANDED)</td>\n",
       "      <td>Computer Science; Optics; Telecommunications</td>\n",
       "      <td>HX8BE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-05</td>\n",
       "      <td>WOS:000467630000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>J</td>\n",
       "      <td>Schwalm, FD; Zandavalli, RB; de Castro, ED; Lu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Schwalm, Fabio Duarte; Zandavalli, Rafaela Bru...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Is there a relationship between spirituality/r...</td>\n",
       "      <td>JOURNAL OF HEALTH PSYCHOLOGY</td>\n",
       "      <td>...</td>\n",
       "      <td>Psychology, Clinical</td>\n",
       "      <td>Social Science Citation Index (SSCI)</td>\n",
       "      <td>Psychology</td>\n",
       "      <td>0F9KM</td>\n",
       "      <td>33499688.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-05</td>\n",
       "      <td>WOS:000628934600001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  PT                                                 AU   BA   BE   GP  \\\n",
       "0  J  Sun, XY; Dai, XY; Yang, TS; Song, HT; Yang, JL...  NaN  NaN  NaN   \n",
       "1  J                             Ladanyi, A; Cinkler, T  NaN  NaN  NaN   \n",
       "2  J  Schwalm, FD; Zandavalli, RB; de Castro, ED; Lu...  NaN  NaN  NaN   \n",
       "\n",
       "                                                  AF   BF   CA  \\\n",
       "0  Sun, Xinyang; Dai, Xuyan; Yang, Tingshu; Song,...  NaN  NaN   \n",
       "1                      Ladanyi, Akos; Cinkler, Tibor  NaN  NaN   \n",
       "2  Schwalm, Fabio Duarte; Zandavalli, Rafaela Bru...  NaN  NaN   \n",
       "\n",
       "                                                  TI  \\\n",
       "0  Effects of mental resilience on neuroendocrine...   \n",
       "1  Resilience-throughput-power trade-off in futur...   \n",
       "2  Is there a relationship between spirituality/r...   \n",
       "\n",
       "                                SO  ...  \\\n",
       "0                        ENDOCRINE  ...   \n",
       "1  PHOTONIC NETWORK COMMUNICATIONS  ...   \n",
       "2     JOURNAL OF HEALTH PSYCHOLOGY  ...   \n",
       "\n",
       "                                                  WC  \\\n",
       "0                         Endocrinology & Metabolism   \n",
       "1  Computer Science, Information Systems; Optics;...   \n",
       "2                               Psychology, Clinical   \n",
       "\n",
       "                                               WE  \\\n",
       "0  Science Citation Index Expanded (SCI-EXPANDED)   \n",
       "1  Science Citation Index Expanded (SCI-EXPANDED)   \n",
       "2            Social Science Citation Index (SSCI)   \n",
       "\n",
       "                                             SC     GA          PM      OA  \\\n",
       "0                    Endocrinology & Metabolism  AU1UG  24633577.0     NaN   \n",
       "1  Computer Science; Optics; Telecommunications  HX8BE         NaN  hybrid   \n",
       "2                                    Psychology  0F9KM  33499688.0     NaN   \n",
       "\n",
       "    HC   HP          DA                   UT  \n",
       "0  NaN  NaN  2023-08-05  WOS:000345404900029  \n",
       "1  NaN  NaN  2023-08-05  WOS:000467630000004  \n",
       "2  NaN  NaN  2023-08-05  WOS:000628934600001  \n",
       "\n",
       "[3 rows x 71 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.shape)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 99758 entries, 0 to 499\n",
      "Data columns (total 71 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   PT      99758 non-null  object \n",
      " 1   AU      99604 non-null  object \n",
      " 2   BA      943 non-null    object \n",
      " 3   BE      8570 non-null   object \n",
      " 4   GP      5412 non-null   object \n",
      " 5   AF      99604 non-null  object \n",
      " 6   BF      943 non-null    object \n",
      " 7   CA      375 non-null    object \n",
      " 8   TI      99758 non-null  object \n",
      " 9   SO      99758 non-null  object \n",
      " 10  SE      9335 non-null   object \n",
      " 11  BS      37 non-null     object \n",
      " 12  LA      99256 non-null  object \n",
      " 13  DT      99744 non-null  object \n",
      " 14  CT      13655 non-null  object \n",
      " 15  CY      13653 non-null  object \n",
      " 16  CL      13653 non-null  object \n",
      " 17  SP      10597 non-null  object \n",
      " 18  HO      1965 non-null   object \n",
      " 19  DE      75498 non-null  object \n",
      " 20  ID      72131 non-null  object \n",
      " 21  AB      88773 non-null  object \n",
      " 22  C1      96663 non-null  object \n",
      " 23  C3      90904 non-null  object \n",
      " 24  RP      93203 non-null  object \n",
      " 25  EM      84762 non-null  object \n",
      " 26  RI      48959 non-null  object \n",
      " 27  OI      61391 non-null  object \n",
      " 28  FU      44850 non-null  object \n",
      " 29  FP      44684 non-null  object \n",
      " 30  FX      43591 non-null  object \n",
      " 31  CR      94087 non-null  object \n",
      " 32  NR      99751 non-null  object \n",
      " 33  TC      99754 non-null  float64\n",
      " 34  Z9      99753 non-null  object \n",
      " 35  U1      99750 non-null  object \n",
      " 36  U2      99729 non-null  object \n",
      " 37  PU      99231 non-null  object \n",
      " 38  PI      99237 non-null  object \n",
      " 39  PA      99234 non-null  object \n",
      " 40  SN      84140 non-null  object \n",
      " 41  EI      69358 non-null  object \n",
      " 42  BN      13799 non-null  object \n",
      " 43  J9      93533 non-null  object \n",
      " 44  JI      84742 non-null  object \n",
      " 45  PD      73850 non-null  object \n",
      " 46  PY      99733 non-null  object \n",
      " 47  VL      84874 non-null  object \n",
      " 48  IS      61952 non-null  object \n",
      " 49  PN      788 non-null    object \n",
      " 50  SU      3687 non-null   object \n",
      " 51  SI      7475 non-null   object \n",
      " 52  MA      1981 non-null   object \n",
      " 53  BP      71751 non-null  object \n",
      " 54  EP      71744 non-null  object \n",
      " 55  AR      24037 non-null  object \n",
      " 56  DI      84330 non-null  object \n",
      " 57  DL      84329 non-null  object \n",
      " 58  D2      2009 non-null   object \n",
      " 59  EA      18828 non-null  object \n",
      " 60  PG      99228 non-null  object \n",
      " 61  WC      99176 non-null  object \n",
      " 62  WE      99228 non-null  object \n",
      " 63  SC      99175 non-null  object \n",
      " 64  GA      99228 non-null  object \n",
      " 65  PM      28471 non-null  float64\n",
      " 66  OA      42742 non-null  object \n",
      " 67  HC      927 non-null    object \n",
      " 68  HP      927 non-null    object \n",
      " 69  DA      99727 non-null  object \n",
      " 70  UT      99727 non-null  object \n",
      "dtypes: float64(2), object(69)\n",
      "memory usage: 54.8+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()\n",
    "# can also try:\n",
    "## data.describe()\n",
    "## data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Export full dataframe into a .csv file\n",
    "\n",
    "Skip to step #6 if you only want to export a subsetted version of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdir = Path(\"data\")\n",
    "data.to_csv(Path(outputdir,\"merged_wos_files.csv\"), encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Subset dataframe and then export\n",
    "\n",
    "Often, the Web of Science database provides more data fields than we need. We can work with a smaller version of the dataset by only keeping those columns we really want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PT', 'AU', 'BA', 'BE', 'GP', 'AF', 'BF', 'CA', 'TI', 'SO', 'SE', 'BS',\n",
       "       'LA', 'DT', 'CT', 'CY', 'CL', 'SP', 'HO', 'DE', 'ID', 'AB', 'C1', 'C3',\n",
       "       'RP', 'EM', 'RI', 'OI', 'FU', 'FP', 'FX', 'CR', 'NR', 'TC', 'Z9', 'U1',\n",
       "       'U2', 'PU', 'PI', 'PA', 'SN', 'EI', 'BN', 'J9', 'JI', 'PD', 'PY', 'VL',\n",
       "       'IS', 'PN', 'SU', 'SI', 'MA', 'BP', 'EP', 'AR', 'DI', 'DL', 'D2', 'EA',\n",
       "       'PG', 'WC', 'WE', 'SC', 'GA', 'PM', 'OA', 'HC', 'HP', 'DA', 'UT'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the data fields (columns in this case) available for Web of Science data. You can see a [full List of WoS data fields here.](https://docs.google.com/spreadsheets/d/1KPNVIrhwZJrqYOsu3jzpRF7pzHCjRVy6K8eu3qTu-cA/edit#gid=1397269035) \n",
    "\n",
    "Then, choose which columns you would like to keep by placing the two-letter column name in the list below.\n",
    "\n",
    "*Place this information somewhere public so I don't have to link to an institutional drive!!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = [\"PT\", #pub type\n",
    "                \"AU\", \"AF\", #author / author full names\n",
    "                \"TI\",  #author title\n",
    "                \"SO\",  #source title\n",
    "                \"LA\",  #language\n",
    "                \"DT\",   #document type\n",
    "                \"DE\", \"ID\",  #author keywords / keywords plus\n",
    "                \"AB\",      #abstract\n",
    "                \"RI\", \"OI\",  #research ids / ORCIDs\n",
    "                \"CR\", #cited references\n",
    "                \"TC\", \"Z9\", \"U1\", \"U2\", #times cited (WoS core) / times cited, all / 180 days usage ct / since 2013 usage count\n",
    "                \"HC\", \"HP\", #highly cited status / hot paper status\n",
    "                \"PU\",  #publisher\n",
    "                \"SN\", \"EI\", \"BN\", \"DI\", \"UT\",   #ISSN / eISSN / ISBN / DOI / WoS id\n",
    "                \"JI\", #journal ISO abbreviation\n",
    "                \"PD\", \"PY\",   #pub data / pub year\n",
    "                \"WC\", \"SC\"  #WoS Categories / Research Areas\n",
    "                ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can create a new dataframe (\"subdata\") with only the columns we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99758, 30)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PT</th>\n",
       "      <th>AU</th>\n",
       "      <th>AF</th>\n",
       "      <th>TI</th>\n",
       "      <th>SO</th>\n",
       "      <th>LA</th>\n",
       "      <th>DT</th>\n",
       "      <th>DE</th>\n",
       "      <th>ID</th>\n",
       "      <th>AB</th>\n",
       "      <th>...</th>\n",
       "      <th>SN</th>\n",
       "      <th>EI</th>\n",
       "      <th>BN</th>\n",
       "      <th>DI</th>\n",
       "      <th>UT</th>\n",
       "      <th>JI</th>\n",
       "      <th>PD</th>\n",
       "      <th>PY</th>\n",
       "      <th>WC</th>\n",
       "      <th>SC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>J</td>\n",
       "      <td>Sun, XY; Dai, XY; Yang, TS; Song, HT; Yang, JL...</td>\n",
       "      <td>Sun, Xinyang; Dai, Xuyan; Yang, Tingshu; Song,...</td>\n",
       "      <td>Effects of mental resilience on neuroendocrine...</td>\n",
       "      <td>ENDOCRINE</td>\n",
       "      <td>English</td>\n",
       "      <td>Article</td>\n",
       "      <td>Mental resilience; Sleep deprivation; Rennin; ...</td>\n",
       "      <td>CORTISOL</td>\n",
       "      <td>The aim of this study was to investigate the e...</td>\n",
       "      <td>...</td>\n",
       "      <td>1355-008X</td>\n",
       "      <td>1559-0100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1007/s12020-014-0228-8</td>\n",
       "      <td>WOS:000345404900029</td>\n",
       "      <td>Endocrine</td>\n",
       "      <td>DEC</td>\n",
       "      <td>2014</td>\n",
       "      <td>Endocrinology &amp; Metabolism</td>\n",
       "      <td>Endocrinology &amp; Metabolism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>J</td>\n",
       "      <td>Ladanyi, A; Cinkler, T</td>\n",
       "      <td>Ladanyi, Akos; Cinkler, Tibor</td>\n",
       "      <td>Resilience-throughput-power trade-off in futur...</td>\n",
       "      <td>PHOTONIC NETWORK COMMUNICATIONS</td>\n",
       "      <td>English</td>\n",
       "      <td>Article</td>\n",
       "      <td>5G; Resilience; Availability; Fixed-mobile con...</td>\n",
       "      <td>WIRELESS</td>\n",
       "      <td>5G New Radio allows operators to use new and w...</td>\n",
       "      <td>...</td>\n",
       "      <td>1387-974X</td>\n",
       "      <td>1572-8188</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1007/s11107-019-00842-2</td>\n",
       "      <td>WOS:000467630000004</td>\n",
       "      <td>Photonic Netw. Commun.</td>\n",
       "      <td>JUN</td>\n",
       "      <td>2019</td>\n",
       "      <td>Computer Science, Information Systems; Optics;...</td>\n",
       "      <td>Computer Science; Optics; Telecommunications</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  PT                                                 AU  \\\n",
       "0  J  Sun, XY; Dai, XY; Yang, TS; Song, HT; Yang, JL...   \n",
       "1  J                             Ladanyi, A; Cinkler, T   \n",
       "\n",
       "                                                  AF  \\\n",
       "0  Sun, Xinyang; Dai, Xuyan; Yang, Tingshu; Song,...   \n",
       "1                      Ladanyi, Akos; Cinkler, Tibor   \n",
       "\n",
       "                                                  TI  \\\n",
       "0  Effects of mental resilience on neuroendocrine...   \n",
       "1  Resilience-throughput-power trade-off in futur...   \n",
       "\n",
       "                                SO       LA       DT  \\\n",
       "0                        ENDOCRINE  English  Article   \n",
       "1  PHOTONIC NETWORK COMMUNICATIONS  English  Article   \n",
       "\n",
       "                                                  DE        ID  \\\n",
       "0  Mental resilience; Sleep deprivation; Rennin; ...  CORTISOL   \n",
       "1  5G; Resilience; Availability; Fixed-mobile con...  WIRELESS   \n",
       "\n",
       "                                                  AB  ...         SN  \\\n",
       "0  The aim of this study was to investigate the e...  ...  1355-008X   \n",
       "1  5G New Radio allows operators to use new and w...  ...  1387-974X   \n",
       "\n",
       "          EI   BN                          DI                   UT  \\\n",
       "0  1559-0100  NaN   10.1007/s12020-014-0228-8  WOS:000345404900029   \n",
       "1  1572-8188  NaN  10.1007/s11107-019-00842-2  WOS:000467630000004   \n",
       "\n",
       "                       JI   PD    PY  \\\n",
       "0               Endocrine  DEC  2014   \n",
       "1  Photonic Netw. Commun.  JUN  2019   \n",
       "\n",
       "                                                  WC  \\\n",
       "0                         Endocrinology & Metabolism   \n",
       "1  Computer Science, Information Systems; Optics;...   \n",
       "\n",
       "                                             SC  \n",
       "0                    Endocrinology & Metabolism  \n",
       "1  Computer Science; Optics; Telecommunications  \n",
       "\n",
       "[2 rows x 30 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdata = data.loc[:, cols_to_keep]\n",
    "print(subdata.shape)   #print the new dimensions of the dataframe\n",
    "subdata.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the dataframe to a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdata.to_csv(Path(outputdir, \"merged-wos_subcols.csv\"), encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create a random sample and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand1000 = subdata.sample(n = 1000)\n",
    "rand1000.to_csv(Path(outputdir, \"merged_wos_rand1000.csv\"), encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
