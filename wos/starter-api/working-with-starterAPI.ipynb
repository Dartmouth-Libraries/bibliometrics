{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with the Web of Science Starter API\n",
    "\n",
    "This notebook demonstrates some standard means for accessing and retrieving data from the Web of Science (WoS) database using the WoS Starter API.\n",
    "\n",
    "The Starter API allows users to automate searches of the WoS database, retrieving the following data:\n",
    "+ author(s), editor(s), etc.\n",
    "+ document title\n",
    "+ source / publication title\n",
    "+ document type\n",
    "+ author keywords <!--[?]-->\n",
    "+ document identifiers (ISSN, eISSN, ISBN, DOI, PubMed Id, Web of Science identifier [UT])\n",
    "+ author /researcher identifiers\n",
    "+ publication year\n",
    "+ volume and issue\n",
    "+ pages\n",
    "+ times cited??\n",
    "\n",
    "*Note: The Starter API also retrieves records using the following fields (but which are not included in the results):*\n",
    "+ keywords:\n",
    "    <!--+ author keywords-->\n",
    "    + Keywords Plus\n",
    "+ Abstract\n",
    "\n",
    "*Thus, you may search for all documents that use \"nuclear disarmament\" in their keywords or abstract, but the returned results will not include the keywords or abstract themselves. If you want to examine how a particular text uses this term you will have to visit the WoS record to review its abstract or use one of the other methods available tot the Dartmouth community for accessing WoS [detailed here](https://researchguides.dartmouth.edu/c.php?g=59725&p=9910244).*\n",
    "\n",
    "### Limits on Starter API use\n",
    "\n",
    "Institutional subscriptions to the Starter API (which Dartmouth Library has) allow researchers to:\n",
    "+ place up to 5,000 requests per day\n",
    "+ up to 5 requests per second\n",
    "+ retrieve up to 50 records per request\n",
    "\n",
    "meaning a researcher can retrieve a maximum of 50,000 records in a given day.\n",
    "\n",
    "For a more detailed comparison of the WoS Starter and Lite APIs and different ways to access WoS data - particularly relevant for Dartmouth community members, please see the [Dartmouth Library *Accessing Web of Science Data* Guide](https://researchguides.dartmouth.edu/c.php?g=59725&p=9910244). For more on the database fields that may be searched and those that will be returned by the Starter API, see the [README available with the Starter API Client Github page](https://github.com/clarivate/wosstarter_python_client/blob/master/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Getting Started\n",
    "\n",
    "1. **Import all necessary packages.** To first install these packages to a local environment, you can use the requirements.txt file. Open a terminal / command prompt within this project's folder and type:\n",
    "\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Then you can import these packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import os\n",
    "import urllib.parse\n",
    "import pandas as pd\n",
    "import random\n",
    "from bs4 import BeautifulSoup   #for parsing xml and html\n",
    "from random import randint  \n",
    "from dotenv import load_dotenv \n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Store the API link to memory and retrieve your API key. See this repository's ReadME.md [**INSERT LINK**] for more on saving and retrieving your API Key from an .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASEURL_ST = 'https://api.clarivate.com/apis/wos-starter/v1/'\n",
    "HEADERS_ST = {'X-APIKey': os.getenv(\"APIKEY\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Search Queries\n",
    "\n",
    "### IIa. Retrieve documents by author\n",
    "\n",
    "3. We can begin with a simple search query for one author. For example, we can search amongst the [suspiciously prolific publications by the Spanish scholar Rafael Luque](https://english.elpais.com/science-tech/2023-04-02/one-of-the-worlds-most-cited-scientists-rafael-luque-suspended-without-pay-for-13-years.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEARCH_QUERY = 'AU=Lepore, Jill' \n",
    "SEARCH_QUERY = \"AU=Luque, Rafael\" # Enter your search query here, in this case we are looking for an author named Rafael Luque\n",
    "#SEARCH_QUERY = 'AU=Schnell, JD'  \n",
    "\n",
    "#SEARCH_QUERY = \"AI=IMS-5344-2023\"\n",
    "#SEARCH_QUERY = \"AI=EFQ-9500-2022\"\n",
    "#SEARCH_QUERY = \"UT=WOS:000188058500010\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **RETRIEVE DATA**: We can then insert the API url (BASEURL_ST), the search query, and the API Key (HEADERS_ST) into a requests command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_request = requests.get(f'{BASEURL_ST}documents?db=WOS&q={urllib.parse.quote(SEARCH_QUERY)}', headers=HEADERS_ST)\n",
    "data = initial_request.json()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[limits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_request = requests.get(f'{BASEURL_ST}documents?db=WOS&q={urllib.parse.quote(SEARCH_QUERY)}&limit=50', headers=HEADERS_ST)\n",
    "data = initial_request.json()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above, however, return the work of multiple scholars with the name Rafael Luque or R. Luque. Instead we need to search for this particular scholar by using his ResearcherID.\n",
    "\n",
    "We can filter the data already retrieved to disambiguate the different Rafael Luques. \n",
    "\n",
    "Let's first examine the data returned (under the name `data`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def author_summary(jsondict):\n",
    "    auth_rows = []\n",
    "    hits = [hit for hit in jsondict['hits']]\n",
    "    print(f\"Titles included in response to the query: '{SEARCH_QUERY}' - {len(hits)}\")\n",
    "    for hit in hits:\n",
    "        title = hit['title']\n",
    "        year = hit['source']['publishYear']\n",
    "        authors = hit['names']['authors']\n",
    "        au_keywords = hit['keywords']['authorKeywords']\n",
    "        #print(authors)\n",
    "        for author in authors:\n",
    "            auth_rows.append([title, year, au_keywords, author['wosStandard'], author['researcherId']])\n",
    "    return(auth_rows)    \n",
    "rows = author_summary(data)\n",
    "\n",
    "author_df = pd.DataFrame(rows, columns = [\"title\", \"year\", \"author_keywords\", \"authorname_wos_std\", \"researcherid\"])\n",
    "print(author_df.shape)\n",
    "author_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEARCH_QUERY = 'RI=F-9853-2010'\n",
    "#type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3, 4]\n",
    "b = [[1, 2, 3], [2, 3, 5], [2,6,7], [2, 3, 8]]\n",
    "c = [[1, 2, 3], [2, 3, 5], [2,6,7], 5]\n",
    "\n",
    "def flatten_extend(matrix: list):\n",
    "    flat_list = []\n",
    "    for row in matrix:\n",
    "        if type(row) is list:\n",
    "            flat_list.extend(row)\n",
    "        else:\n",
    "            flat_list.append(row)\n",
    "    return flat_list\n",
    "\n",
    "flatten_extend(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby researcherid\n",
    "from collections import Counter\n",
    "#author_df.groupby(by = \"researcherid\")[\"author_keywords\"].apply(list)\n",
    "\n",
    "def flatten_extend(matrix: list):\n",
    "    flat_list = []\n",
    "    for row in matrix:\n",
    "        if type(row) is list:\n",
    "            flat_list.extend(row)\n",
    "        else:\n",
    "            flat_list.append(row)\n",
    "    return flat_list\n",
    "\n",
    "def most_common_keywords(kwlist: list):\n",
    "    #print(kwlist)\n",
    "    flat_kwlist = flatten_extend(kwlist)\n",
    "    #print(\"FLATTENED:\", flat_kwlist)\n",
    "    kw_ctr = Counter(flat_kwlist)\n",
    "    #print(\"***counter:\", kw_ctr)\n",
    "    kw_most_common = kw_ctr.most_common(5)\n",
    "    #print(\"$$Top 3 most common:\", kw_most_common)\n",
    "    return kw_most_common\n",
    "\n",
    "author_summary = author_df.groupby(by = \"researcherid\")\\\n",
    "    .agg({\"authorname_wos_std\": \"first\", \"title\": \"count\", \"author_keywords\": lambda x: most_common_keywords(x)})\n",
    "\n",
    "author_summary = author_summary.sort_values(by = \"title\", ascending=False)\n",
    "author_summary_sub = author_summary[author_summary['authorname_wos_std'].str.startswith(\"Luque\")]\n",
    "author_summary_sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **VIEW SUMMARY INFORMATION FROM WOS REQUEST**: As you can see, this returns a json document as a Python dictionary. We can retrieve specific information from this record the same way we would a dictionary. To retrieve only a summary of the record, we can run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['metadata']  #returns total number of records "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve only summary information for each publication (title, source, year, and page range) we can run the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_summary(jsondict):\n",
    "    hits = [hit for hit in jsondict['hits']]\n",
    "    print(f\"Titles included in response to the query: '{SEARCH_QUERY}'\")\n",
    "    for hit in hits:\n",
    "        title = hit['title']\n",
    "        so_title = hit['source']['sourceTitle']\n",
    "        year = hit['source']['publishYear']\n",
    "        pages = hit['source']['pages']['range'].split(\"-\")\n",
    "        print(f\"{year}. '{title}', {so_title}: {pages[0]}-{pages[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_summary(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **RECORD LIMITS**: Although this query finds 50 records, only 10 are returned due to the default limit of records. Let's change that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_request = requests.get(f'{BASEURL_ST}documents?db=WOS&q={urllib.parse.quote(SEARCH_QUERY)}' f'&limit=50', headers=HEADERS_ST)\n",
    "data = initial_request.json()\n",
    "data['metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_summary(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IIb. Other Search Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. **SEARCHING THE WOS DATASET USING OTHER FIELDS**: We can search by a variety of fields besides author (\"AU\") and even combine searches by multiple fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search fields for the Web of Science Starter API are listed in the [project's README page](https://github.com/clarivate/wosstarter_python_client) but are also copied here: \n",
    "\n",
    "| Field Tag | Description                                                                                                                                                 |\n",
    "|-----------|-------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| TI        | Title of document                                                                                                                                           |\n",
    "| IS        | ISSN or ISBN                                                                                                                                                |\n",
    "| SO        | Source title - The result contains all source titles within product database (for example, journal titles and/or book titles if the product includes books) |\n",
    "| VL        | Volume                                                                                                                                                      |\n",
    "| PG        | Page                                                                                                                                                        |\n",
    "| CS        | Issue                                                                                                                                                       |\n",
    "| PY        | Year Published                                                                                                                                              |\n",
    "| AU        | Author                                                                                                                                                      |\n",
    "| AI        | Author Identifier                                                                                                                                                      |\n",
    "| UT        | Accession Number                                                                                                                                            |\n",
    "| DO        | DOI                                                                                                                                                         |\n",
    "| DT        | [Document Type](https://webofscience.help.clarivate.com/en-us/Content/document-types.html)                                                                                                                                                         |\n",
    "| PMID      | PubMed ID                                                                                                                                                   |\n",
    "| OG        | Search for preferred organization names and/or their name variants from the Preferred Organization Index. <p> A search on a preferred organization name returns all records that contain the preferred name and all records that contain its name variants. A search on a name variant returns all records that contain the variant. For example, Cornell Law Sch returns all records that contain Cornell Law Sch in the Addresses field. <p> When searching for organization names that contain a Boolean (AND, NOT, NEAR, and SAME), always enclose the word in quotation marks ( \\\" \\\" ). For example: <p>   - OG=(Japan Science \\\"and\\\" Technology Agency (JST))      <br>   - OG=(\\\"Near\\\" East Univ)         <br> - OG=(\\\"OR\\\" Hlth Sci Univ)                           |\n",
    "| TS        | Searches for topic terms in the following fields within a document: <p> - Title <br> - Abstract <br> - Author keywords <br> - Keywords Plus\n",
    "\n",
    "\n",
    "Allowed tags are AI, AU, CS, DO, DT, IS, OG, PG, PMID, PY, SO, SUR, TI, TS, UT, VL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IIc. Search by Publication\n",
    "\n",
    "8. **SEARCH FOR A SPECIFIC JOURNAL**: You may also, for example, search by journal. We recommend looking up the journal's ISSN number rather than searching by journal title. You can look up a journal's ISSN using the [ISSN Portal](https://portal.issn.org/).\n",
    "\n",
    "For example, we can search in ***The William and Mary Quarterly*** using its ISSN (\"0043-5597\"). This query identifies 6,082 results (although it only returns 10\\*). \n",
    "\n",
    "*\\*Note: we are keeping the 'limit' at 10 for this experimentation. But, if you want the API to retrieve the maximum allowed number of sources (50), add in \"&limit=50\" as we did in Step #6 above. If we wanted to retrieve all 6,082 results, for example, we could do so using a loop that first retrieves records 1-50, then 51-100, and so on for 122 times until it finishes. Part III shows how to do so below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_QUERY = \"IS=0043-5597\"\n",
    "initial_request = requests.get(f'{BASEURL_ST}documents?db=WOS&q={urllib.parse.quote(SEARCH_QUERY)}', headers=HEADERS_ST)\n",
    "initial_request.json()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. **FILTER BY YEAR(S)**: We may then narrow our search of this one journal by limiting it to a series of years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_QUERY = \"IS=0043-5597 AND PY=(2000-2010)\"\n",
    "initial_request = requests.get(f'{BASEURL_ST}documents?db=WOS&q={urllib.parse.quote(SEARCH_QUERY)}', headers=HEADERS_ST)\n",
    "initial_request.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. **FILTER BY DOCUMENT TYPE**: We can filter by Document Type (DT) to retrieve only articles (rather than book reviews and other types) from this journal. As you can see, we have reduced the number of results to 222."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_QUERY = \"IS=0043-5597 AND PY=(2000-2010) AND DT=Article\"\n",
    "initial_request = requests.get(f'{BASEURL_ST}documents?db=WOS&q={urllib.parse.quote(SEARCH_QUERY)}', headers=HEADERS_ST)\n",
    "\n",
    "initial_request.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IId. Search by keyword\n",
    "\n",
    "11. **TOPIC TERMS SEARCH**: We can also use the Starter API's Topic Terms (TS) field to search for keywords and terms found in a text's Title, Author Keywords, Keywords Plus, and Abstract fields. As noted in the introduction to this notebook, while you can search through these fields, only the Title field is actually returned. \n",
    "\n",
    "In the example below, we retrieve all works that have the term \"humanized landscape\" stored in these fields. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEARCH_QUERY = \"TS=(ecology AND humanized landscape)\"\n",
    "SEARCH_QUERY = \"TS=humanized landscape\"\n",
    "initial_request = requests.get(f'{BASEURL_ST}documents?db=WOS&q={urllib.parse.quote(SEARCH_QUERY)}', headers=HEADERS_ST)\n",
    "data = initial_request.json()\n",
    "data['metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Placing Retrieved Data in a Dataframe and saving to a csv\n",
    "\n",
    "The Starter API returns query results in the format of [JSON data](https://www.w3schools.com/js/js_json_intro.asp), which is hierarchical in format. \n",
    "\n",
    "Often, however, researchers will want to place this data in a two-dimensional data table (in Python - these data tables are known as dataframes). This part shows how to transform this returned JSON data into a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. **Retrieve 50 or fewer records and place in a dataframe**\n",
    "\n",
    "12a. We will continue with our most recent SEARCH_QUERY. First, let's retrieve the number of records this query found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Our current search query: {SEARCH_QUERY}\")\n",
    "total_records = data['metadata']['total']\n",
    "print(total_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12b. In a later step, we will work on retrieving all 306 records. For the moment, however, let's just request the first 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#50 or less results\n",
    "datadict = {}\n",
    "initial_request = requests.get(\n",
    "        f'{BASEURL_ST}documents?db=WOS&q={SEARCH_QUERY}&limit=50', headers=HEADERS_ST)\n",
    "data = initial_request.json()\n",
    "datadict = data\n",
    "print(f\"Total number of records pulled: {len(datadict['hits'])}\")\n",
    "print(f\"for the search query: {SEARCH_QUERY}\")\n",
    "uids = set([hit['uid'] for hit in datadict['hits']])\n",
    "print(f\"Total number of unique ids: {len(uids)}\")\n",
    "print(f\"Number of requests remaining today: {initial_request.headers['X-RateLimit-Remaining-Day']}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12c. The code above retrieved the data (\"initial_request\"), saved it as a json file (\"data\"), created an empty dictionary (\"datadict\"), placed the JSON data in this dictionary, and then created a list of unique IDs found in the dictionary (\"uids\").\n",
    "\n",
    "Let's examine the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(uids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadict.keys()\n",
    "#there are two keys in this dictionary: 'metadata' and 'hits'. \n",
    "## Let's examine the values stored with the 'hits' key more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(datadict['hits'])\n",
    "# Information stored under the 'hits' key is stored in a list. \n",
    "# Let's examine how many items are in the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(datadict['hits'])\n",
    "#... and the results stored within the first item in the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadict['hits'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12d. Now that we have a better idea how data is returned by the WOS Starter API, we can create a function that converts the JSON results (which we have stored in a Python dictionary) into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_all_data(datahits):\n",
    "    #hits = [hit for hit in data['hits']]\n",
    "    datalist = []\n",
    "    for hit in datahits:\n",
    "        datadict = {}\n",
    "        datadict['uid'] = hit.get(\"uid\", \"\")\n",
    "        datadict['title'] = hit.get(\"title\", \"\")\n",
    "        datadict['authors'] = \"; \".join([name.get('wosStandard') for name in hit.get(\"names\").get(\"authors\")])\n",
    "        datadict['researcherIds'] = \"; \".join([str(name.get('researcherId')) for name in hit.get(\"names\").get(\"authors\")])\n",
    "        datadict['pubyear'] = hit.get(\"source\").get(\"publishYear\")\n",
    "        datadict['source_title'] = hit.get(\"source\").get(\"sourceTitle\")\n",
    "        datadict['volume'] = hit.get(\"source\").get(\"volume\")\n",
    "        datadict['page_start'] = hit.get(\"source\").get(\"pages\").get(\"begin\")\n",
    "        datadict['page_end'] = hit.get(\"source\").get(\"pages\").get(\"end\")\n",
    "        datadict['page_count'] = hit.get(\"source\").get(\"pages\").get(\"count\")\n",
    "        identifiers = hit.get(\"identifiers\")\n",
    "        datadict['doi'] = identifiers.get(\"doi\")\n",
    "        datadict['issn'] = identifiers.get(\"issn\")\n",
    "        datadict['eissn'] = identifiers.get(\"eissn\")\n",
    "        datadict['isbn'] = identifiers.get(\"isbn\")\n",
    "        citations = hit.get(\"citations\")\n",
    "        if len(citations) > 0:   #for some reason the citations key stores a dict inside a list\n",
    "            datadict[\"citation_counts\"] = citations[0].get(\"count\")\n",
    "         \n",
    "        datadict['author_keywords'] = \"; \".join([kw.lower() for kw in hit.get(\"keywords\").get(\"authorKeywords\")])\n",
    "        #datadict['keywords_plus'] = hit.get(\"keywords\").get(\"keywordsPlus\")\n",
    "        links = hit.get(\"links\")\n",
    "        datadict['record_links'] = links.get(\"record\")\n",
    "        datadict['citing_links'] = links.get(\"citingArticles\")\n",
    "        datadict['reference_links'] = links.get(\"references\")\n",
    "        datadict['related_links'] = links.get(\"related\")\n",
    "        datalist.append(datadict)\n",
    "    #print(datalist)\n",
    "    return(pd.DataFrame(datalist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12e. With this function we can now convert our dictionary (\"datadict\") into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = retrieve_all_data(datadict['hits'])\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12f. ...and export this dataframe into a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write results to csv\n",
    "df1.to_csv(f\"wos_results_{SEARCH_QUERY}_page1only.csv\", encoding = 'utf=8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. **SEND MULTIPLE REQUESTS TO RETRIEVE 50+ RECORDS FROM A QUERY**:\n",
    "\n",
    "However, our search query (\"TS=humanized landscapes\") returned more than 50 results (306 in total).\n",
    "\n",
    "When a query returns more than 50 records, we need to write a iterative loop that retrieves 50 records at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13a. Create a for loop to retrieve 50 records at a time and assemble the result into one Python dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_records = data['metadata']['total']\n",
    "print(f\"Our current search query: {SEARCH_QUERY} returned {total_records} records.\")\n",
    "\n",
    "requests_required = ((total_records - 1) // 50) + 1  #306 records - 1 = 305 // 50 = 6 + 1 = 7\n",
    "print(requests_required)\n",
    "datadict = {}\n",
    "if requests_required > 1:\n",
    "    print(f\"API requests required to get all data from the query - '{SEARCH_QUERY}': {requests_required}\")\n",
    "for i in range(requests_required):\n",
    "    subsequent_response = requests.get(\n",
    "        f'{BASEURL_ST}documents?db=WOS&q={SEARCH_QUERY}&limit=50&page={i+1}', headers=HEADERS_ST)\n",
    "    data = subsequent_response.json()\n",
    "    if i == 0:\n",
    "        print(data['metadata'])\n",
    "        datadict = data\n",
    "    else:\n",
    "        datadict['hits'].extend(data['hits'])\n",
    "    print(f\"**Pulling from Page {i+1} of {requests_required}**\")\n",
    "print(f\"Total number of records pulled: {len(datadict['hits'])}\")\n",
    "uids = set([hit['uid'] for hit in datadict['hits']])\n",
    "print(f\"Total number of unique ids: {len(uids)}\")\n",
    "print(f\"Number of requests remaining today: {subsequent_response.headers['X-RateLimit-Remaining-Day']}.\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13b. Next, we can transform the dictionary into a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = retrieve_all_data(datadict['hits'])\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13c. ... and export it as a csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv(f\"wos_results_{SEARCH_QUERY}_all.csv\", encoding = 'utf=8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. We can review the number of requests we have remaining for today:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsequent_response.headers['X-RateLimit-Remaining-Day']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
